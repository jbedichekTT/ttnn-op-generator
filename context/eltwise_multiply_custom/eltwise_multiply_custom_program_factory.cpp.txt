Required APIs (6):
  - tt::tt_metal::operation::ProgramWithCallbacks

  - tt::tt_metal::Program

  - tt::tt_metal::CreateKernel

  - tt::tt_metal::CreateCircularBuffer

  - tt::tt_metal::SetRuntimeArgs

  - tt::tt_metal::CoreRangeSet
    ✓ Validated in: ttnn/api/ttnn/types.hpp


Required Includes (4):
  - tt_metal/host_api.hpp
    Reason: Core TT-Metal APIs for Program, CreateKernel, etc.
    Expected: Program, CreateKernel, CreateCircularBuffer, SetRuntimeArgs
  - ttnn/api/ttnn/types.hpp
    Reason: TTNN type definitions including CoreRangeSet
    Expected: CoreRangeSet, Tensor
  - ttnn/tensor/tensor.hpp
    Reason: Tensor type and related operations
    Expected: Tensor, tensor properties
  - ttnn/operations/eltwise_multiply_custom/device/eltwise_multiply_custom_program_factory.hpp
    Reason: Header declaration for this program factory
    Expected: eltwise_multiply_custom_multi_core

Namespace Imports:
  - using namespace tt::tt_metal;
  - using namespace tt::tt_metal::operation;
  - using namespace ttnn;

Key Patterns to Follow:
  - Program factory functions return tt::tt_metal::operation::ProgramWithCallbacks
  - Use multi-core patterns for element-wise operations
  - Create reader kernel for input tensor A reading
  - Create reader kernel for input tensor B reading
  - Create compute kernel for element-wise multiplication
  - Create writer kernel for output tensor writing
  - Use circular buffers for data flow: src0_cb, src1_cb, dst0_cb
  - Configure core grid based on tensor dimensions and device capabilities
  - Set up runtime arguments for tensor addresses and shapes
  - Handle both interleaved and sharded tensor layouts

Common Mistakes to Avoid:
  - Don't forget to create all required circular buffers (src0, src1, dst0)
  - Ensure kernel file paths match the actual generated kernel files
  - Properly configure core ranges to avoid resource conflicts
  - Set correct data formats for circular buffers
  - Include proper error handling for invalid tensor configurations
  - Use const& for input tensor parameters
  - Ensure circular buffer sizes are adequate for tile/block sizes


================================================================================
FINAL GENERATION PROMPT
================================================================================

Generate the code for the file `device/eltwise_multiply_custom_program_factory.cpp` for the `eltwise_multiply_custom` operation.


        # TT-Metal Program Factory Development Context

        ## Program Factory Role & Responsibilities

        ### Core Purpose
        Program factories are **orchestration engines** that:
        - **Create and configure programs** that coordinate multiple kernels
        - **Manage memory layouts** and circular buffer configurations
        - **Dispatch work** across single or multiple cores
        - **Handle runtime arguments** and tensor shape adaptations
        - **Optimize execution** based on tensor properties and hardware constraints

        IMPORTANT: The kernels called in the program factory must be the readers and writers generated, not any existing kernels.
        This means the correct kernels are named `{operation_name}_reader.cpp`, `<operation_name>_writer.cpp`, and `<operation_name>_compute.cpp`.
        **Include Path Context for TTNN Operations:**


        VALIDATED API INFORMATION:
        - tt::tt_metal::CoreRangeSet
  Include: ttnn/api/ttnn/types.hpp
  Purpose: Specify core ranges for kernel execution


        REQUIRED HEADERS:
        #include <tt-metalium/host_api.hpp>
  // Core TT-Metal APIs for Program, CreateKernel, etc.
#include "ttnn/api/ttnn/types.hpp"
  // TTNN type definitions including CoreRangeSet
  // Provides: 9 constants, 2 namespaces, 1 structs, 15 usings
#include "ttnn/tensor/tensor.hpp"
  // Tensor type and related operations
  // Provides: 1 classes, 1 constants, 23 functions, 4 namespaces, 4 template_functions
#include "ttnn/operations/eltwise_multiply_custom/device/eltwise_multiply_custom_program_factory.hpp"
  // Header declaration for this program factory

        NAMESPACE USAGE:
        tt::tt_metal, tt::tt_metal::operation, ttnn
        
Key Implementation Patterns:
- Program factory functions return tt::tt_metal::operation::ProgramWithCallbacks
- Use only multi-core patterns for element-wise operations.
- Create reader kernel for input tensor A reading
- Create reader kernel for input tensor B reading
- Create compute kernel for element-wise multiplication
- Create writer kernel for output tensor writing
- Use circular buffers for data flow: src0_cb, src1_cb, dst0_cb
- Configure core grid based on tensor dimensions and device capabilities
- Set up runtime arguments for tensor addresses and shapes
- Handle both interleaved and sharded tensor layouts
        
Common Mistakes to Avoid:
- Don't forget to create all required circular buffers (src0, src1, dst0)
- Ensure kernel file paths match the actual generated kernel files
- Properly configure core ranges to avoid resource conflicts
- Set correct data formats for circular buffers
- Include proper error handling for invalid tensor configurations
- Use const& for input tensor parameters
- Ensure circular buffer sizes are adequate for tile/block sizes
.

Here is a reference implementation for a multi-core program factory in the matmul operation:


// SPDX-FileCopyrightText: © 2023 Tenstorrent AI ULC
//
// SPDX-License-Identifier: Apache-2.0

#include <tt-metalium/constants.hpp>
#include <tt-metalium/util.hpp>
#include <tt-metalium/host_api.hpp>
#include <tt-metalium/work_split.hpp>
#include "ttnn/operations/matmul/device/matmul_op.hpp"

using namespace tt;
using namespace tt::constants;

namespace ttnn {

namespace operations {

namespace matmul {

tt::tt_metal::operation::ProgramWithCallbacks matmul_multi_core(
    const Tensor& a, const Tensor& b, Tensor& output, bool bcast_batch) {
    tt_metal::Program program{};

    const auto &ashape = a.get_padded_shape(), bshape = b.get_padded_shape();

    tt::DataFormat in0_data_format = tt_metal::datatype_to_dataformat_converter(a.get_dtype());
    tt::DataFormat in1_data_format = tt_metal::datatype_to_dataformat_converter(b.get_dtype());
    tt::DataFormat output_data_format = tt_metal::datatype_to_dataformat_converter(output.get_dtype());
    uint32_t in0_single_tile_size = tt_metal::detail::TileSize(in0_data_format);
    uint32_t in1_single_tile_size = tt_metal::detail::TileSize(in1_data_format);
    uint32_t output_single_tile_size = tt_metal::detail::TileSize(output_data_format);
    MathFidelity math_fidelity = MathFidelity::HiFi4;

    tt_metal::Buffer* src0_buffer = a.buffer();
    tt_metal::Buffer* src1_buffer = b.buffer();

    // This should allocate a DRAM buffer on the device
    tt::tt_metal::IDevice* device = a.device();
    const auto& cshape = output.get_padded_shape();  // C=A*B, N1MK*11KN->N1MN

    auto compute_with_storage_grid_size = device->compute_with_storage_grid_size();
    uint32_t num_cores_x = compute_with_storage_grid_size.x;
    uint32_t num_cores_y = compute_with_storage_grid_size.y;
    uint32_t c_batch_size = get_batch_size(cshape);
    auto num_output_tiles_total = c_batch_size * cshape[-2] * cshape[-1] / TILE_HW;
    auto
        [num_cores,
         all_cores,
         core_group_1,
         core_group_2,
         num_output_tiles_per_core_group_1,
         num_output_tiles_per_core_group_2] =
            tt::tt_metal::split_work_to_cores(compute_with_storage_grid_size, num_output_tiles_total);

    tt_metal::Buffer* dst_buffer = output.buffer();
    TT_FATAL(dst_buffer != nullptr, "Output buffer should be allocated on device!");

    // C = A*B*...
    // MN = MK*KN
    uint32_t B = get_batch_size(ashape);
    uint32_t Mt = ashape[-2] / TILE_HEIGHT;
    uint32_t Kt = ashape[-1] / TILE_WIDTH;
    uint32_t Nt = bshape[-1] / TILE_WIDTH;
    uint32_t KtNt = Kt * Nt;
    uint32_t MtKt = Mt * Kt;
    uint32_t MtNt = Mt * Nt;

    uint32_t src0_addr = src0_buffer->address();
    uint32_t src1_addr = src1_buffer->address();
    uint32_t dst_addr = dst_buffer->address();

    uint32_t src0_cb_index = 0;
    uint32_t num_input_tiles = 2;
    tt_metal::CircularBufferConfig src0_cb_config =
        tt_metal::CircularBufferConfig(num_input_tiles * in0_single_tile_size, {{src0_cb_index, in0_data_format}})
            .set_page_size(src0_cb_index, in0_single_tile_size);
    auto cb_src0 = tt_metal::CreateCircularBuffer(program, all_cores, src0_cb_config);

    uint32_t src1_cb_index = 1;
    tt_metal::CircularBufferConfig src1_cb_config =
        tt_metal::CircularBufferConfig(num_input_tiles * in1_single_tile_size, {{src1_cb_index, in1_data_format}})
            .set_page_size(src1_cb_index, in1_single_tile_size);
    auto cb_src1 = tt_metal::CreateCircularBuffer(program, all_cores, src1_cb_config);

    uint32_t output_cb_index = tt::CBIndex::c_16;
    uint32_t num_output_tiles = 2;
    tt_metal::CircularBufferConfig output_cb_config =
        tt_metal::CircularBufferConfig(
            num_output_tiles * output_single_tile_size, {{output_cb_index, output_data_format}})
            .set_page_size(output_cb_index, output_single_tile_size);
    auto cb_output = tt_metal::CreateCircularBuffer(program, all_cores, output_cb_config);

    bool src0_is_dram = src0_buffer->buffer_type() == tt_metal::BufferType::DRAM;
    bool src1_is_dram = src1_buffer->buffer_type() == tt_metal::BufferType::DRAM;
    uint32_t last_ktile_w = a.get_logical_shape()[-1] % TILE_WIDTH;
    std::vector<uint32_t> reader_compile_time_args = {
        (uint32_t)src0_is_dram, (uint32_t)src1_is_dram, (uint32_t)last_ktile_w};

    bool dst_is_dram = dst_buffer->buffer_type() == tt_metal::BufferType::DRAM;
    std::vector<uint32_t> writer_compile_time_args = {(std::uint32_t)output_cb_index, (std::uint32_t)dst_is_dram};

    auto reader_id = tt_metal::CreateKernel(
        program,
        "ttnn/cpp/ttnn/operations/matmul/device/kernels/dataflow/reader_bmm_8bank_output_tiles_partitioned.cpp",
        all_cores,
        tt_metal::ReaderDataMovementConfig(reader_compile_time_args));

    auto writer_id = tt_metal::CreateKernel(
        program,
        "ttnn/cpp/ttnn/operations/eltwise/unary/device/kernels/dataflow/writer_unary_interleaved_start_id.cpp",
        all_cores,
        tt_metal::WriterDataMovementConfig(writer_compile_time_args));

    std::vector<uint32_t> compute_args_group_1 = {
        1,                                 // B
        1,                                 // Mt
        Kt,                                // Kt
        num_output_tiles_per_core_group_1  // Nt
    };  // bmm compute kernel the B, Mt, Nt are just 3 for loops that technically act as 1 large loop, so only set Nt
        // for simplicity

    auto eltwise_binary_kernel_group_1_id = tt_metal::CreateKernel(
        program,
        "ttnn/cpp/ttnn/operations/matmul/device/kernels/compute/bmm.cpp",
        core_group_1,
        tt_metal::ComputeConfig{
            .math_fidelity = math_fidelity, .dst_full_sync_en = true, .compile_args = compute_args_group_1});

    if (!core_group_2.ranges().empty()) {
        std::vector<uint32_t> compute_args_group_2 = {
            1,                                 // B
            1,                                 // Mt
            Kt,                                // Kt
            num_output_tiles_per_core_group_2  // Nt
        };  // bmm compute kernel the B, Mt, Nt are just 3 for loops that technically act as 1 large loop, so only set
            // Nt for simplicity

        auto eltwise_binary_kernel_group_2_id = tt_metal::CreateKernel(
            program,
            "ttnn/cpp/ttnn/operations/matmul/device/kernels/compute/bmm.cpp",
            core_group_2,
            tt_metal::ComputeConfig{
                .math_fidelity = math_fidelity, .dst_full_sync_en = true, .compile_args = compute_args_group_2});
    }

    for (uint32_t i = 0, num_tiles_written = 0; i < num_cores; i++) {
        CoreCoord core = {i / num_cores_y, i % num_cores_y};

        uint32_t num_output_tiles_per_core = 0;
        if (core_group_1.contains(core)) {
            num_output_tiles_per_core = num_output_tiles_per_core_group_1;
        } else if (core_group_2.contains(core)) {
            num_output_tiles_per_core = num_output_tiles_per_core_group_2;
        } else {
            TT_THROW("Core not in specified core ranges");
        }
        tt_metal::SetRuntimeArgs(
            program,
            reader_id,
            core,
            {src0_addr,
             src1_addr,
             Mt,
             Kt,
             Nt,
             MtKt,
             KtNt,
             B,
             uint32_t(bcast_batch),
             num_tiles_written,
             num_output_tiles_per_core,
             MtNt});
        tt_metal::SetRuntimeArgs(program, writer_id, core, {dst_addr, num_output_tiles_per_core, num_tiles_written});
        num_tiles_written += num_output_tiles_per_core;
    }

    auto override_runtime_args_callback =
        [reader_kernel_id = reader_id, writer_kernel_id = writer_id, num_cores, num_cores_y](
            const void* operation,
            const Program& program,
            const std::vector<Tensor>& input_tensors,
            const std::vector<std::optional<const Tensor>>&,
            const std::vector<Tensor>& output_tensors) {
            auto src_dram_buffer_a = input_tensors.at(0).buffer();
            auto src_dram_buffer_b = input_tensors.at(1).buffer();

            auto dst_dram_buffer = output_tensors.at(0).buffer();

            for (uint32_t i = 0, num_tiles_written = 0; i < num_cores; i++) {
                CoreCoord core = {i / num_cores_y, i % num_cores_y};

                {
                    auto& runtime_args = GetRuntimeArgs(program, reader_kernel_id, core);
                    runtime_args[0] = src_dram_buffer_a->address();
                    runtime_args[1] = src_dram_buffer_b->address();
                }

                {
                    auto& runtime_args = GetRuntimeArgs(program, writer_kernel_id, core);
                    runtime_args[0] = dst_dram_buffer->address();
                }
            }
        };

    return {.program = std::move(program), .override_runtime_arguments_callback = override_runtime_args_callback};
}

}  // namespace matmul

}  // namespace operations

}  // namespace ttnn

Use only the validated APIs and includes in the header file, do not add any new ones.  Generate the full program factory code. 
